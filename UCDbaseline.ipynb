{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/cwang/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/cwang/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import string\n",
    "import sklearn\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as VS\n",
    "\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "\n",
    "import spacy as sp\n",
    "nlp = sp.load('en')\n",
    "\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "sentiment_analyzer = VS()\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "enStop = stopwords.words('english')\n",
    "enStop_dict={e: 0 for e in enStop}\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# load oov dictionary for word correction\n",
    "oov_dict={}\n",
    "with open(\"OOV_Dictionary_V1.0.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        oov_dict[line.split()[0]]=line.strip().split()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model (Muhammad, 2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.2 s, sys: 3.79 s, total: 1min 1s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "# The pre-trained model can be downloaed from: https://crisisnlp.qcri.org/lrec2016/lrec2016.html\n",
    "%time model = gensim.models.KeyedVectors.load_word2vec_format('crisisNLP_word_vector.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# \"Request-GoodsServices\",\n",
    "# \"Request-SearchAndRescue\",\n",
    "# \"CallToAction-MovePeople\",\n",
    "# \"Report-EmergingThreats\", \n",
    "# \"Report-NewSubEvent\", \n",
    "# \"Report-ServiceAvailable\"\n",
    "# \"\"\"\n",
    "def load_actionable_verbs(important_list={'GoodsServices':0,'SearchAndRescue':0,'MovePeople':0,'EmergingThreats':0,'NewSubEvent':0,'ServiceAvailable':0}): \n",
    "    actionnable_verb_list_dict={}\n",
    "    df_actionable=pd.read_csv(\"actionable_tweets.csv\",sep=\"\\t\")\n",
    "    for index,row in df_actionable.iterrows():\n",
    "        tweet_text=row['text']\n",
    "        category=row['categories']\n",
    "        if category in important_list:\n",
    "            if category not in actionnable_verb_list_dict:\n",
    "                actionnable_verb_list_dict[category]=[]\n",
    "            doc=nlp(tweet_text)\n",
    "            verbs=[token.text for token in doc if token.pos_ == \"VERB\"]\n",
    "            for v in verbs:\n",
    "                if len(v)>=3 and v not in enStop_dict:\n",
    "                    actionnable_verb_list_dict[category].append(v)\n",
    "    return {e:sorted(nltk.FreqDist(actionnable_verb_list_dict[e]).items(), key=lambda kv: kv[1],reverse=True) for e in actionnable_verb_list_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.8 s, sys: 412 ms, total: 18.2 s\n",
      "Wall time: 18.2 s\n"
     ]
    }
   ],
   "source": [
    "%time sorted_actionnable_verb_dict=load_actionable_verbs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['MovePeople', 'SearchAndRescue', 'EmergingThreats', 'ServiceAvailable', 'GoodsServices'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_actionnable_verb_dict2dict_top10={}\n",
    "for event in sorted_actionnable_verb_dict:\n",
    "    sorted_actionnable_verb_dict2dict_top10[event]={}\n",
    "    for tup in sorted_actionnable_verb_dict[event][:10]:\n",
    "        sorted_actionnable_verb_dict2dict_top10[event][stemmer.stem(tup[0])]=tup[1]\n",
    "sorted_actionnable_verb_dict2dict_top10.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('leave', 4), ('coming', 4), ('affected', 3), ('evacuated', 3), ('evacuating', 3), ('evacuate', 3), ('flash', 2), ('take', 2), ('highparkfire', 2), ('use', 2)]\n",
      "[('hits', 22), ('bigwet', 20), ('reported', 18), ('BREAKING', 14), ('killed', 13), ('says', 13), ('nswfires', 12), ('strikes', 11), ('injured', 11), ('killing', 11)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('missing', 37),\n",
       " ('help', 17),\n",
       " ('needs', 14),\n",
       " ('trapped', 11),\n",
       " ('stranded', 9),\n",
       " ('NEED', 7),\n",
       " ('stuck', 6),\n",
       " ('send', 6),\n",
       " ('need', 5),\n",
       " ('Missing', 4)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sorted_actionnable_verb_dict['MovePeople'][:10])\n",
    "print(sorted_actionnable_verb_dict['EmergingThreats'][:10])\n",
    "sorted_actionnable_verb_dict['SearchAndRescue'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopList = [\"http\", \"https\", \"rt\", \"@\", \":\", \"t.co\", \"co\", \"amp\", \"&amp;\", \"...\", \"\\n\", \"\\r\"]\n",
    "stopList.extend(string.punctuation)\n",
    "\n",
    "local_tokenizer = TweetTokenizer()\n",
    "def tokenizer_wrapper(text):\n",
    "    return local_tokenizer.tokenize(text)\n",
    "\n",
    "#define vectorizer using sklearn\n",
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(\n",
    "    tokenizer=tokenizer_wrapper,\n",
    "    ngram_range=(1, 1),\n",
    "    stop_words=stopList, #We do better when we keep stopwords\n",
    "    use_idf=True,\n",
    "    smooth_idf=False,\n",
    "    norm=None, #Applies l2 norm smoothing\n",
    "    decode_error='replace',\n",
    "    max_features=10000,\n",
    "    min_df=4,\n",
    "    max_df=0.501)\n",
    "\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "def normalize(s):\n",
    "        \"\"\"\n",
    "        Given a text, cleans and normalizes it. Feel free to add your own stuff.\n",
    "        From: https://www.kaggle.com/mschumacher/using-fasttext-models-for-robust-embeddings\n",
    "        \"\"\"\n",
    "        s = s.lower()\n",
    "        # Replace numbers and symbols with language\n",
    "        s = s.replace('&', ' and ')\n",
    "        s = s.replace('@', ' at ')\n",
    "        s = s.replace('0', 'zero')\n",
    "        s = s.replace('1', 'one')\n",
    "        s = s.replace('2', 'two')\n",
    "        s = s.replace('3', 'three')\n",
    "        s = s.replace('4', 'four')\n",
    "        s = s.replace('5', 'five')\n",
    "        s = s.replace('6', 'six')\n",
    "        s = s.replace('7', 'seven')\n",
    "        s = s.replace('8', 'eight')\n",
    "        s = s.replace('9', 'nine')\n",
    "        return s\n",
    "\n",
    "def tokenize_tweet(string):\n",
    "    string=string.lower()\n",
    "    # Clean and Refine (remove URL, special characters)\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    string = re.sub(giant_url_regex, \"\", string)\n",
    "    \n",
    "    string = re.sub(r\"\\'s\", \"s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \"ve\", string)\n",
    "    string = re.sub(r\"\\'t\", \"t\", string)\n",
    "    string = re.sub(r\"\\'re\", \"re\", string)\n",
    "    string = re.sub(r\"\\'d\", \"d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \"ll\", string)\n",
    "    \n",
    "    #Skip retweet signs, @ symbols, and special chars such as punctuations\n",
    "    string = re.sub(r\"[^A-Za-z]\", \" \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    tokens=[]\n",
    "    for w in string.split():\n",
    "        # OOV Correction\n",
    "        if w in oov_dict:\n",
    "            w=oov_dict[w]\n",
    "        # Short words skip and Stop word removal\n",
    "        if len(w) >= 3 and w not in enStop_dict:\n",
    "            tokens.append(w)\n",
    "    return tokens\n",
    "\n",
    "def vectorize(sentence):\n",
    "    tokenized = tokenize_tweet(sentence)\n",
    "    wvs = []\n",
    "    for t in tokenized:\n",
    "        if t in model:\n",
    "            v = model[t]\n",
    "            norm = np.linalg.norm(v)\n",
    "            normed_v = v / norm\n",
    "            wvs.append(normed_v) \n",
    "    m = np.array(wvs)\n",
    "    normed_m = np.mean(m, axis=0)\n",
    "    return normed_m\n",
    "\n",
    "# construct X matrix\n",
    "def to_matrix(raw_tweet_texts):\n",
    "    X_matrix = np.zeros((len(raw_tweet_texts), 300))\n",
    "    for index,s in enumerate(raw_tweet_texts):\n",
    "        sv = vectorize(s)\n",
    "        if not np.isnan(sv).any():\n",
    "            X_matrix[index,:]=sv\n",
    "        else:\n",
    "            print(index,s)\n",
    "    return X_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset without meta-info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:\n",
      " trecis2018train\ttrecis2018test\ttrescis2019atest\n",
      " 1337\t17653\t7098\n",
      "------------\n",
      "Total:\n",
      " 26088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cwang/.local/lib/python3.5/site-packages/ipykernel_launcher.py:5: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>post_id</th>\n",
       "      <th>text</th>\n",
       "      <th>categories</th>\n",
       "      <th>priority</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albertaFloods2013</td>\n",
       "      <td>351734622884855808</td>\n",
       "      <td>#yycflood be sure when you are ready for re bu...</td>\n",
       "      <td>ThirdPartyObservation,Advice</td>\n",
       "      <td>Low</td>\n",
       "      <td>11 Sep 2018 19:36:19 GMT</td>\n",
       "      <td>2018test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>albertaFloods2013</td>\n",
       "      <td>351741648327294977</td>\n",
       "      <td>Happy Canada Day everyone! Let's show national...</td>\n",
       "      <td>FirstPartyObservation,Advice</td>\n",
       "      <td>Low</td>\n",
       "      <td>11 Sep 2018 19:36:32 GMT</td>\n",
       "      <td>2018test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>albertaFloods2013</td>\n",
       "      <td>351776825967513600</td>\n",
       "      <td>Unloading a truck filled with clean-up kits in...</td>\n",
       "      <td>ThirdPartyObservation,ContinuingNews,Sentiment</td>\n",
       "      <td>Low</td>\n",
       "      <td>11 Sep 2018 19:36:49 GMT</td>\n",
       "      <td>2018test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>albertaFloods2013</td>\n",
       "      <td>352043285876977664</td>\n",
       "      <td>Unofficial “anthem” of #Alberta floods by #Can...</td>\n",
       "      <td>FirstPartyObservation,Advice</td>\n",
       "      <td>Low</td>\n",
       "      <td>11 Sep 2018 19:37:12 GMT</td>\n",
       "      <td>2018test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>albertaFloods2013</td>\n",
       "      <td>352054501466849280</td>\n",
       "      <td>Get the Alberta Strong (Flood Montage) as hear...</td>\n",
       "      <td>ThirdPartyObservation,Advice</td>\n",
       "      <td>Low</td>\n",
       "      <td>11 Sep 2018 19:37:23 GMT</td>\n",
       "      <td>2018test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            event_id             post_id  \\\n",
       "0  albertaFloods2013  351734622884855808   \n",
       "1  albertaFloods2013  351741648327294977   \n",
       "2  albertaFloods2013  351776825967513600   \n",
       "3  albertaFloods2013  352043285876977664   \n",
       "4  albertaFloods2013  352054501466849280   \n",
       "\n",
       "                                                text  \\\n",
       "0  #yycflood be sure when you are ready for re bu...   \n",
       "1  Happy Canada Day everyone! Let's show national...   \n",
       "2  Unloading a truck filled with clean-up kits in...   \n",
       "3  Unofficial “anthem” of #Alberta floods by #Can...   \n",
       "4  Get the Alberta Strong (Flood Montage) as hear...   \n",
       "\n",
       "                                       categories priority  \\\n",
       "0                    ThirdPartyObservation,Advice      Low   \n",
       "1                    FirstPartyObservation,Advice      Low   \n",
       "2  ThirdPartyObservation,ContinuingNews,Sentiment      Low   \n",
       "3                    FirstPartyObservation,Advice      Low   \n",
       "4                    ThirdPartyObservation,Advice      Low   \n",
       "\n",
       "                  timestamp    source  \n",
       "0  11 Sep 2018 19:36:19 GMT  2018test  \n",
       "1  11 Sep 2018 19:36:32 GMT  2018test  \n",
       "2  11 Sep 2018 19:36:49 GMT  2018test  \n",
       "3  11 Sep 2018 19:37:12 GMT  2018test  \n",
       "4  11 Sep 2018 19:37:23 GMT  2018test  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in order to run the cell, download three datasets from trecis2018-train,trecis2018-test, trescis2019a-test respectively.\n",
    "# covert them to tsv file with format as shown in the output\n",
    "data_dir=\"raw/\"\n",
    "trecis2018train=pd.read_csv(data_dir+\"trecis2018-train.tsv\",sep=\"\\t\")\n",
    "trecis2018train['timestamp']='null'\n",
    "idx=trecis2018train[trecis2018train['priority'].str.contains(\"Unknown\")].index\n",
    "trecis2018train.set_value(idx, 'priority', \"Low\")\n",
    "trecis2018train['source']='2018train'\n",
    "trecis2018test=pd.read_csv(data_dir+\"trecis2018-test.tsv\",sep=\"\\t\")\n",
    "trecis2018test['source']='2018test'\n",
    "trescis2019atest=pd.read_csv(data_dir+\"trescis2019a-test.tsv\",sep=\"\\t\")\n",
    "trescis2019atest['source']='2019a'\n",
    "print(\"Dataset size:\\n trecis2018train\\ttrecis2018test\\ttrescis2019atest\\n {}\\t{}\\t{}\".format(len(trecis2018train),len(trecis2018test),len(trescis2019atest)))\n",
    "print(\"------------\")\n",
    "print(\"Total:\\n {}\".format(len(trecis2018train)+len(trecis2018test)+len(trescis2019atest)))\n",
    "trecis2018test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26088, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>post_id</th>\n",
       "      <th>text</th>\n",
       "      <th>categories</th>\n",
       "      <th>priority</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26083</th>\n",
       "      <td>fireYMM2016D</td>\n",
       "      <td>727632069682044928</td>\n",
       "      <td>@JHauk84 @stephen_taylor Our prayers are with ...</td>\n",
       "      <td>[Hashtags, Sentiment, Discussion]</td>\n",
       "      <td>Low</td>\n",
       "      <td>8 Apr 2019 16:50:15 GMT</td>\n",
       "      <td>2019a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26084</th>\n",
       "      <td>fireYMM2016D</td>\n",
       "      <td>727636108729720832</td>\n",
       "      <td>To think the lead headline at noon on @GlobalC...</td>\n",
       "      <td>[Hashtags, Sentiment]</td>\n",
       "      <td>Low</td>\n",
       "      <td>8 Apr 2019 18:39:43 GMT</td>\n",
       "      <td>2019a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26085</th>\n",
       "      <td>fireYMM2016D</td>\n",
       "      <td>727635394670383105</td>\n",
       "      <td>Just found out my friends in #ymmfire is safe,...</td>\n",
       "      <td>[Hashtags, Sentiment]</td>\n",
       "      <td>Low</td>\n",
       "      <td>8 Apr 2019 18:05:13 GMT</td>\n",
       "      <td>2019a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26086</th>\n",
       "      <td>fireYMM2016D</td>\n",
       "      <td>727629223616249858</td>\n",
       "      <td>RT @BreannaCTV: A lot of people are hanging ar...</td>\n",
       "      <td>[MultimediaShare, Hashtags, News]</td>\n",
       "      <td>Low</td>\n",
       "      <td>5 Apr 2019 17:15:39 GMT</td>\n",
       "      <td>2019a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26087</th>\n",
       "      <td>fireYMM2016D</td>\n",
       "      <td>727629552009220097</td>\n",
       "      <td>RT @puravida_lisa: Terrifying - get out safe e...</td>\n",
       "      <td>[MultimediaShare, Hashtags, OriginalEvent, Sen...</td>\n",
       "      <td>Low</td>\n",
       "      <td>5 Apr 2019 18:57:24 GMT</td>\n",
       "      <td>2019a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           event_id             post_id  \\\n",
       "26083  fireYMM2016D  727632069682044928   \n",
       "26084  fireYMM2016D  727636108729720832   \n",
       "26085  fireYMM2016D  727635394670383105   \n",
       "26086  fireYMM2016D  727629223616249858   \n",
       "26087  fireYMM2016D  727629552009220097   \n",
       "\n",
       "                                                    text  \\\n",
       "26083  @JHauk84 @stephen_taylor Our prayers are with ...   \n",
       "26084  To think the lead headline at noon on @GlobalC...   \n",
       "26085  Just found out my friends in #ymmfire is safe,...   \n",
       "26086  RT @BreannaCTV: A lot of people are hanging ar...   \n",
       "26087  RT @puravida_lisa: Terrifying - get out safe e...   \n",
       "\n",
       "                                              categories priority  \\\n",
       "26083                  [Hashtags, Sentiment, Discussion]      Low   \n",
       "26084                              [Hashtags, Sentiment]      Low   \n",
       "26085                              [Hashtags, Sentiment]      Low   \n",
       "26086                  [MultimediaShare, Hashtags, News]      Low   \n",
       "26087  [MultimediaShare, Hashtags, OriginalEvent, Sen...      Low   \n",
       "\n",
       "                     timestamp source  \n",
       "26083  8 Apr 2019 16:50:15 GMT  2019a  \n",
       "26084  8 Apr 2019 18:39:43 GMT  2019a  \n",
       "26085  8 Apr 2019 18:05:13 GMT  2019a  \n",
       "26086  5 Apr 2019 17:15:39 GMT  2019a  \n",
       "26087  5 Apr 2019 18:57:24 GMT  2019a  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some changes of information types happened with the evolution of TREC-IS, so this is used for reformating information types\n",
    "trecis2018train[\"categories\"]=trecis2018train['categories'].apply(lambda x: x.replace(\"PastNews\",\"ContextualInformation\") if \"PastNews\" in x else x)\n",
    "trecis2018train[\"categories\"]=trecis2018train['categories'].apply(lambda x: x.replace(\"ContinuingNews\",\"News\") if \"ContinuingNews\" in x else x)\n",
    "trecis2018train[\"categories\"]=trecis2018train['categories'].apply(lambda x: x.replace(\"KnownAlready\",\"OriginalEvent\") if \"KnownAlready\" in x else x)\n",
    "trecis2018train[\"categories\"]=trecis2018train['categories'].apply(lambda x: x.replace(\"SignificantEventChange\",\"NewSubEvent\") if \"SignificantEventChange\" in x else x)\n",
    "trecis2018train[\"categories\"]=trecis2018train['categories'].apply(lambda x: x.replace(\"Unknown\",\"Irrelevant\") if \"Unknown\" in x else x)\n",
    "\n",
    "trecis2018test[\"categories\"]=trecis2018test['categories'].apply(lambda x: x.replace(\"PastNews\",\"ContextualInformation\") if \"PastNews\" in x else x)\n",
    "trecis2018test[\"categories\"]=trecis2018test['categories'].apply(lambda x: x.replace(\"ContinuingNews\",\"News\") if \"ContinuingNews\" in x else x)\n",
    "trecis2018test[\"categories\"]=trecis2018test['categories'].apply(lambda x: x.replace(\"KnownAlready\",\"OriginalEvent\") if \"KnownAlready\" in x else x)\n",
    "trecis2018test[\"categories\"]=trecis2018test['categories'].apply(lambda x: x.replace(\"SignificantEventChange\",\"NewSubEvent\") if \"SignificantEventChange\" in x else x)\n",
    "trecis2018test[\"categories\"]=trecis2018test['categories'].apply(lambda x: x.replace(\"Unknown\",\"Irrelevant\") if \"Unknown\" in x else x)\n",
    "\n",
    "df_combined=pd.concat([trecis2018train,trecis2018test,trescis2019atest])\n",
    "df_combined=df_combined.reset_index(drop=True)\n",
    "df_combined[\"categories\"]=df_combined['categories'].apply(lambda x: x.split(\",\"))\n",
    "print(df_combined.shape)\n",
    "df_combined.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Typhoon #RubyPH will make a landfall in Dolores, Eastern Samar by 8 pm. Head to higher ground. http://t.co/Fv0ehCiClE http://t.co/b7LrWD2n2P\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'typhoon rubyph make landfall dolores eastern samar head higher ground'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(getTweetTextsbyIT('MovePeople')[1])\n",
    "\" \".join(tokenize_tweet(getTweetTextsbyIT('MovePeople')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTweetTextsbyIT(it=\"MovePeople\"):\n",
    "    return df_combined[df_combined['categories'].astype(str).str.contains(it)].reset_index(drop=True)['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I hate you\n",
      "Noun phrases: ['I', 'you']\n",
      "Verbs: ['hate']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/cwang/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'compound': 0.3612, 'neg': 0.0, 'neu': 0.286, 'pos': 0.714}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tweet=getTweetTextsbyIT('MovePeople')[1]\n",
    "test_tweet=\"I hate you\"\n",
    "print(test_tweet)\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "doc = nlp(test_tweet)\n",
    "\n",
    "# Analyze syntax\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)\n",
    "    \n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "sentiment_analyzer = VS()\n",
    "\n",
    "sentiment = sentiment_analyzer.polarity_scores(\"I like you\")\n",
    "sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bigwet': 20,\n",
       " 'break': 14,\n",
       " 'hit': 22,\n",
       " 'injur': 11,\n",
       " 'kill': 11,\n",
       " 'nswfire': 12,\n",
       " 'report': 18,\n",
       " 'say': 13,\n",
       " 'strike': 11}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_actionnable_verb_dict2dict_top10['EmergingThreats']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Features considered for boosting are as follows:\n",
    "1. No. of hashtags (numeric)\n",
    "2. No. of special verbs, such as, trapped, stuck, move etc (dataset statistical analysis)\n",
    "3. Sentiment polarity (categorical,, -1, 0 or 1)\n",
    "4. Tweet length (word_length, char_length, numeric)\n",
    "5. URL count (numeric)\n",
    "6. Digit count (int, numeric)\n",
    "7. Retweet check (0 or 1)\n",
    "8. caps ratio (float, numeric)\n",
    "9. Special chars count (@, ! and ?, normalized float numeric)\n",
    "10. : in first token check (0 or 1)\n",
    "11. Named Entity count (numeric)\n",
    "\"\"\"\n",
    "def return_features(tweet_text):\n",
    "    #1. No. of hashtags (numeric)\n",
    "    hashtag_count=tweet_text.count(\"#\")\n",
    "    #3. Sentiment polarity (categorical,, -1, 0 or 1)\n",
    "    sentiment = sentiment_analyzer.polarity_scores(tweet_text)\n",
    "    sentiment_cpd=sentiment['compound']\n",
    "    sentiment_neg=sentiment['neg']\n",
    "    sentiment_neu=sentiment['neu']\n",
    "    sentiment_pos=sentiment['pos']\n",
    "  \n",
    "    #2. No. of special verbs such as, trapped, stuck, move etc (dataset statistical analysis)\n",
    "    #['MovePeople', 'EmergingThreats', 'GoodsServices', 'SearchAndRescue', 'ServiceAvailable']\n",
    "    tokens=tokenize_tweet(tweet_text)\n",
    "    is_action1=int(any(stemmer.stem(i) in sorted_actionnable_verb_dict2dict_top10['MovePeople'] for i in tokens))\n",
    "    is_action2=int(any(stemmer.stem(i) in sorted_actionnable_verb_dict2dict_top10['EmergingThreats'] for i in tokens))\n",
    "    is_action3=int(any(stemmer.stem(i) in sorted_actionnable_verb_dict2dict_top10['GoodsServices'] for i in tokens))\n",
    "    is_action4=int(any(stemmer.stem(i) in sorted_actionnable_verb_dict2dict_top10['SearchAndRescue'] for i in tokens))\n",
    "    is_action5=int(any(stemmer.stem(i) in sorted_actionnable_verb_dict2dict_top10['ServiceAvailable'] for i in tokens))\n",
    "    \n",
    "    #4. Tweet length (word_length, char_length, numeric)\n",
    "    tokens=tweet_text.split(\" \")\n",
    "    word_length=len(tokens)\n",
    "    char_length=len(tweet_text)\n",
    "    \n",
    "    #5. URL count (numeric)\n",
    "    url_count=len(re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',tweet_text))\n",
    "    #6. Digit count (int, numeric)\n",
    "    digital_count=len( re.findall('[0-9]+', tweet_text))\n",
    "    \n",
    "    #7. Retweet check (0 or 1)\n",
    "    is_retweet=int(tweet_text[:2]==\"RT\")\n",
    "    # 8. caps ratio (float, numeric)\n",
    "    caps_ratio=len(\"\".join(re.findall('[A-Z]+', tweet_text)))/char_length\n",
    "    \n",
    "    #9. Special chars count\n",
    "    at_count=tweet_text.count(\"@\")\n",
    "    exclaim_count=tweet_text.count(\"!\")\n",
    "    question_count=tweet_text.count(\"?\")\n",
    "    \n",
    "    #10. : in first token check (0 or 1)\n",
    "    colon_check=int(\":\" in tokens[0])\n",
    "    #11. Named Entity count (numeric)\n",
    "    doc = nlp(tweet_text)\n",
    "    ner_count=len(doc.ents)\n",
    "#     print(doc.ents)\n",
    "    \n",
    "    return_list=[hashtag_count,sentiment_cpd,sentiment_neg,sentiment_neu,sentiment_pos,is_action1,\n",
    "                    is_action2,is_action3,is_action4,is_action5,word_length,char_length,\n",
    "                    url_count,digital_count,is_retweet,caps_ratio,at_count,exclaim_count,question_count,\n",
    "                    colon_check,ner_count]\n",
    "    return return_list\n",
    "\n",
    "def normalize_by_columns(crafted_features_matrix,columns_to_normalize=[\"hashtag_count\",\"word_length\",\"char_length\",\"url_count\",\"digital_count\",\"at_count\",\"exclaim_count\",\"question_count\",\"ner_count\"]):\n",
    "    crafted_features_matrix_=crafted_features_matrix.copy()\n",
    "    to_normalize_features_matrix=crafted_features_matrix_[columns_to_normalize]\n",
    "    from sklearn import preprocessing\n",
    "    x = to_normalize_features_matrix.values \n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(x)\n",
    "    normalized_features_matrix = pd.DataFrame(x_scaled,columns=columns_to_normalize)\n",
    "    normalized_features_matrix.head()\n",
    "    for column in columns_to_normalize:\n",
    "        crafted_features_matrix_[column]=normalized_features_matrix[column]\n",
    "    return crafted_features_matrix_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test for  Typhoon #RubyPH will make a landfall in Dolores, Eastern Samar by 8 pm. Head to higher ground. http://t.co/Fv0ehCiClE http://t.co/b7LrWD2n2P\n",
      "hashtag_count ====> 1\n",
      "sentiment_cpd ====> 0.0\n",
      "sentiment_neg ====> 0.0\n",
      "sentiment_neu ====> 1.0\n",
      "sentiment_pos ====> 0.0\n",
      "is_action1 ====> 0\n",
      "is_action2 ====> 0\n",
      "is_action3 ====> 0\n",
      "is_action4 ====> 0\n",
      "is_action5 ====> 0\n",
      "word_length ====> 19\n",
      "char_length ====> 140\n",
      "url_count ====> 2\n",
      "digital_count ====> 5\n",
      "is_retweet ====> 0\n",
      "caps_ratio ====> 0.11428571428571428\n",
      "at_count ====> 0\n",
      "exclaim_count ====> 0\n",
      "question_count ====> 0\n",
      "colon_check ====> 0\n",
      "ner_count ====> 2\n"
     ]
    }
   ],
   "source": [
    "test_tweet=getTweetTextsbyIT('MovePeople')[1]\n",
    "print(\"test for \",test_tweet)\n",
    "feature_names=[\"hashtag_count\",\"sentiment_cpd\",\"sentiment_neg\",\"sentiment_neu\",\n",
    "               \"sentiment_pos\",\"is_action1\",\"is_action2\",\"is_action3\",\"is_action4\",\n",
    "              \"is_action5\",\"word_length\",\"char_length\",\"url_count\",\"digital_count\",\n",
    "              \"is_retweet\",\"caps_ratio\",\"at_count\",\"exclaim_count\",\"question_count\",\n",
    "              \"colon_check\",\"ner_count\"]\n",
    "import datetime\n",
    "start_time=datetime.datetime.now()\n",
    "features=return_features(test_tweet)\n",
    "\n",
    "for idx,fature_name in enumerate(feature_names):\n",
    "    print(fature_name,\"====>\",features[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now is processing at index 0\n",
      "Now is processing at index 1000\n",
      "Now is processing at index 2000\n",
      "Now is processing at index 3000\n",
      "Now is processing at index 4000\n",
      "Now is processing at index 5000\n",
      "Now is processing at index 6000\n",
      "Now is processing at index 7000\n",
      "Now is processing at index 8000\n",
      "Now is processing at index 9000\n",
      "Now is processing at index 10000\n",
      "Now is processing at index 11000\n",
      "Now is processing at index 12000\n",
      "Now is processing at index 13000\n",
      "Now is processing at index 14000\n",
      "Now is processing at index 15000\n",
      "Now is processing at index 16000\n",
      "Now is processing at index 17000\n",
      "Now is processing at index 18000\n",
      "Now is processing at index 19000\n",
      "Now is processing at index 20000\n",
      "Now is processing at index 21000\n",
      "Now is processing at index 22000\n",
      "Now is processing at index 23000\n",
      "Now is processing at index 24000\n",
      "Now is processing at index 25000\n",
      "Now is processing at index 26000\n",
      "(26088, 24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cwang/.local/lib/python3.5/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype object was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>sentiment_cpd</th>\n",
       "      <th>sentiment_neg</th>\n",
       "      <th>sentiment_neu</th>\n",
       "      <th>sentiment_pos</th>\n",
       "      <th>is_action1</th>\n",
       "      <th>is_action2</th>\n",
       "      <th>is_action3</th>\n",
       "      <th>is_action4</th>\n",
       "      <th>...</th>\n",
       "      <th>digital_count</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>caps_ratio</th>\n",
       "      <th>at_count</th>\n",
       "      <th>exclaim_count</th>\n",
       "      <th>question_count</th>\n",
       "      <th>colon_check</th>\n",
       "      <th>ner_count</th>\n",
       "      <th>categories</th>\n",
       "      <th>priority</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>242997841134505985</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0</td>\n",
       "      <td>0.136000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>OriginalEvent</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>243121539552256001</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0</td>\n",
       "      <td>0.123711</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>OriginalEvent</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>243198739882332161</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>OriginalEvent</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>243203924050448384</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0</td>\n",
       "      <td>0.131148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>OriginalEvent</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>243361298518249473</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.3612</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0</td>\n",
       "      <td>0.104478</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>OriginalEvent</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              post_id  hashtag_count  sentiment_cpd  sentiment_neg  \\\n",
       "0  242997841134505985       0.388889         0.0000            0.0   \n",
       "1  243121539552256001       0.055556         0.0000            0.0   \n",
       "2  243198739882332161       0.055556         0.0000            0.0   \n",
       "3  243203924050448384       0.055556         0.0000            0.0   \n",
       "4  243361298518249473       0.000000        -0.3612            0.2   \n",
       "\n",
       "   sentiment_neu  sentiment_pos is_action1 is_action2 is_action3 is_action4  \\\n",
       "0            1.0            0.0          0          1          0          0   \n",
       "1            1.0            0.0          0          0          0          0   \n",
       "2            1.0            0.0          0          0          0          0   \n",
       "3            1.0            0.0          0          0          0          0   \n",
       "4            0.8            0.0          0          1          0          0   \n",
       "\n",
       "   ... digital_count  is_retweet  caps_ratio  at_count  exclaim_count  \\\n",
       "0  ...      0.210526           0    0.136000       0.0            0.0   \n",
       "1  ...      0.368421           0    0.123711       0.0            0.0   \n",
       "2  ...      0.157895           0    0.111111       0.0            0.0   \n",
       "3  ...      0.157895           0    0.131148       0.0            0.0   \n",
       "4  ...      0.105263           0    0.104478       0.0            0.0   \n",
       "\n",
       "  question_count  colon_check  ner_count     categories  priority  \n",
       "0            0.0            0   0.307692  OriginalEvent       Low  \n",
       "1            0.0            0   0.307692  OriginalEvent       Low  \n",
       "2            0.0            1   0.230769  OriginalEvent       Low  \n",
       "3            0.0            0   0.230769  OriginalEvent       Low  \n",
       "4            0.0            0   0.153846  OriginalEvent       Low  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "if os.path.isfile(\"crafted_features_matrix_2018B.csv\"):\n",
    "    crafted_features_matrix=pd.read_csv(\"crafted_features_matrix.csv\")\n",
    "    categories = []\n",
    "    for it in crafted_features_matrix['categories']:\n",
    "        categories.append(it.split(\",\"))\n",
    "    crafted_features_matrix['categories']=categories\n",
    "else:\n",
    "    feature_names=[\"post_id\",\"hashtag_count\",\"sentiment_cpd\",\"sentiment_neg\",\"sentiment_neu\",\n",
    "               \"sentiment_pos\",\"is_action1\",\"is_action2\",\"is_action3\",\"is_action4\",\n",
    "              \"is_action5\",\"word_length\",\"char_length\",\"url_count\",\"digital_count\",\n",
    "              \"is_retweet\",\"caps_ratio\",\"at_count\",\"exclaim_count\",\"question_count\",\n",
    "              \"colon_check\",\"ner_count\",\"categories\",\"priority\"]\n",
    "    crafted_features_matrix=pd.DataFrame(columns=feature_names)\n",
    "    texts=df_combined['text']\n",
    "    postids=df_combined['post_id']\n",
    "    categories=df_combined['categories']\n",
    "    priority=df_combined['priority']\n",
    "    for index,tweet_text in enumerate(texts):\n",
    "        if index%1000==0:\n",
    "            print(\"Now is processing at index\",index)\n",
    "        features=return_features(tweet_text)\n",
    "        features.insert(0,postids[index])\n",
    "        features.append(\",\".join(categories[index]))\n",
    "        features.append(priority[index])\n",
    "        crafted_features_matrix.loc[len(crafted_features_matrix)]=features\n",
    "    crafted_features_matrix.to_csv(\"crafted_features_matrix_2018B.csv\",index=False)\n",
    "    \n",
    "new_crafted_features_matrix=normalize_by_columns(crafted_features_matrix)\n",
    "print(new_crafted_features_matrix.shape)\n",
    "new_crafted_features_matrix.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML models for combined features (crafted features + word2vec features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437 #HighParkFire from I-25. http://t.co/2nXZFfIv\n",
      "5751 Oh my. https://t.co/F0vFbYpfFO\n",
      "5792 What?? https://t.co/gkwuoJP8rJ\n",
      "6183 R.I.P. https://t.co/jyRqACDjoC\n",
      "6233 From @meighanstone https://t.co/JpEY1CxTqt\n",
      "6766 Pa howwwww https://t.co/eb3pSR57Va\n",
      "6778 Where ? https://t.co/za0G2BIr0U\n",
      "7505 Me too... https://t.co/zOUnWhPupr\n",
      "7596 This! https://t.co/kTlSuCuvDB\n",
      "8442 More on this at 6:20 am on @cbcsask https://t.co/RuXyssorVh\n",
      "8481 This https://t.co/dpcfSr6bpd\n",
      "8621 this. https://t.co/tRdssIupJ0\n",
      "8702 What this https://t.co/qoBEYiLgNT\n",
      "9275 this. https://t.co/5iVaLykcuc\n",
      "10316 This. https://t.co/a2qLYC3pqB\n",
      "10477 Me too! https://t.co/rJJssfKljg\n",
      "10529 This this this https://t.co/8l2hvzNo5U\n",
      "10702 Oh no :( https://t.co/DdW9qL7nSu\n",
      "11007 Oh no:-( https://t.co/rYBXHlm2Q2\n",
      "19055 #PrayforBoholandCebu #PrayForBohol #PrayForCarmenBohol ??\n",
      "19216 #prayforBohol http://t.co/lJczvAi5R2\n",
      "19363 RT @TrustJo: #PrayForBohol http://t.co/TOQjOQThkk\n",
      "19458 #PrayForBohol http://t.co/xfyoTri7bs\n",
      "19523 #PrayForBohol http://t.co/7ypYVqZiaQ\n",
      "(26088, 321)\n"
     ]
    }
   ],
   "source": [
    "word2vec_matrix=to_matrix(df_combined['text'])\n",
    "word2vec_matrix.shape\n",
    "combined_matrix=np.concatenate([word2vec_matrix, new_crafted_features_matrix.iloc[:,1:22].values], axis=1)\n",
    "\n",
    "#X and y construction for priortiy classification\n",
    "X_crafted_matrix=combined_matrix\n",
    "print(X_crafted_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priority Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26088, 321)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_crafted_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26088,)\n",
      "[0 0 0 ... 0 0 0]\n",
      "(79672, 321)\n",
      "1\n",
      "CPU times: user 1min 19s, sys: 359 ms, total: 1min 20s\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "priortiy2label={\"Low\":0,\"Unknown\":0,\"Medium\":1,\"High\":2,\"Critical\":3}\n",
    "label2priority={0: 'Low', 1: 'Medium', 2: 'High', 3: 'Critical'}\n",
    "new_crafted_features_matrix_=new_crafted_features_matrix.replace({\"priority\":priortiy2label})\n",
    "y_crafted_pri=new_crafted_features_matrix_['priority'].values\n",
    "print(y_crafted_pri.shape)\n",
    "print(y_crafted_pri)\n",
    "\n",
    "\n",
    "y_crafted_pri=y_crafted_pri.astype('int')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(X_crafted_matrix, \n",
    "                                                                            y_crafted_pri, \n",
    "                                                                            test_size=0.0, \n",
    "                                                                            random_state=0)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "oversampler=SMOTE('not majority')\n",
    "features_train,labels_train=oversampler.fit_sample(features_train,labels_train)\n",
    "print(features_train.shape)\n",
    "print(len(labels_train.shape))\n",
    "\n",
    "# By GridSearch and KFold validation, we get the highest accurancy with parameter C=xx\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# val_clf = LogisticRegression()\n",
    "# C_params = [0.001, 0.01, 0.1, 1, 10,100]\n",
    "# # gamma_params=[0.01,0.1,1,10,100]\n",
    "# param_grid = dict(C=C_params)\n",
    "# kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\n",
    "# grid_search = GridSearchCV(val_clf, param_grid, n_jobs=-1, cv=kfold)\n",
    "# grid_result = grid_search.fit(features_train, labels_train)\n",
    "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "# means = grid_result.cv_results_['mean_test_score']\n",
    "# params = grid_result.cv_results_['params']\n",
    "# for mean, param in zip(means, params):\n",
    "#     print(\"%f  with: %r\" % (mean, param))\n",
    "\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# gnb_pri_model = GaussianNB()\n",
    "# %time gnb_pri_model.fit(features_train, labels_train)\n",
    "# pri_model=gnb_pri_model\n",
    "\n",
    "#poor on critical although hits a higher average precision and recall over GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "# lsvc_pri_model = LinearSVC(C=0.1)\n",
    "lsvc_pri_model = LogisticRegression(C=100)\n",
    "# lsvc_pri_model = SVC(C=0.1,kernel=\"linear\")\n",
    "%time lsvc_pri_model.fit(features_train, labels_train)\n",
    "pri_model=lsvc_pri_model\n",
    "\n",
    "# print('Accuracy of LinearSVC classifier on FULL set with oversampling for priority (Train set): {:.4f}'\n",
    "#      .format(pri_model.score(features_train, labels_train)))\n",
    "# print('Accuracy of LinearSVC classifier on FULL set with oversampling for priority (Test set): {:.4f}'\n",
    "#      .format(pri_model.score(features_test, labels_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_test = pri_model.predict(features_test)\n",
    "# from sklearn.metrics import classification_report\n",
    "# class_names=[label2priority[each] for each in range(0,4)]\n",
    "# print(classification_report(labels_test, predicted_test,target_names=class_names))\n",
    "# print(\"Evaluate on training set before SMOTE\")\n",
    "# from sklearn.metrics import classification_report\n",
    "# class_names_=[label2priority[each] for each in range(0,4)]\n",
    "# print(classification_report(y_crafted_pri, pri_model.predict(X_crafted_matrix),target_names=class_names_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Label classification for information type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITEMS NEEDED! RT@cbmyyc: @innfromthecold needs grocery store gift cards and bus passes! #yycflood\n",
      "['Advice', 'Donations', 'ThirdPartyObservation']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def onehot_to_labels(onehot):\n",
    "    labels=[]\n",
    "    for index,each in enumerate(onehot):\n",
    "        if each == 1:\n",
    "            labels.append(list(multilabel_binarizer.classes_)[index])\n",
    "    return labels\n",
    "\n",
    "def labels_to_onehot(label_list):\n",
    "    onehot=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    for each in label_list:\n",
    "        onehot[list(multilabel_binarizer.classes_).index(each)]=1\n",
    "    return np.array(onehot)\n",
    "\n",
    "def normalize_prob(raw_list=['0','1','2','3']):\n",
    "    raw_list=[float(i) for i in raw_list]\n",
    "    max_raw_score=np.max(raw_list)\n",
    "    min_raw_score=np.min(raw_list)\n",
    "    normalized=[]\n",
    "    for each in raw_list:\n",
    "        normalized.append(each/(max_raw_score-min_raw_score))\n",
    "    return normalized\n",
    "\n",
    "index2label={0: 'Advice',\n",
    " 1: 'CleanUp',\n",
    " 2: 'ContextualInformation',\n",
    " 3: 'Discussion',\n",
    " 4: 'Donations',\n",
    " 5: 'EmergingThreats',\n",
    " 6: 'Factoid',\n",
    " 7: 'FirstPartyObservation',\n",
    " 8: 'GoodsServices',\n",
    " 9: 'Hashtags',\n",
    " 10: 'InformationWanted',\n",
    " 11: 'Irrelevant',\n",
    " 12: 'Location',\n",
    " 13: 'MovePeople',\n",
    " 14: 'MultimediaShare',\n",
    " 15: 'NewSubEvent',\n",
    " 16: 'News',\n",
    " 17: 'Official',\n",
    " 18: 'OriginalEvent',\n",
    " 19: 'SearchAndRescue',\n",
    " 20: 'Sentiment',\n",
    " 21: 'ServiceAvailable',\n",
    " 22: 'ThirdPartyObservation',\n",
    " 23: 'Volunteer',\n",
    " 24: 'Weather'}\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "multilabel_binarizer = MultiLabelBinarizer()\n",
    "multilabel_binarizer.fit(df_combined['categories'])\n",
    "X_matrix=X_crafted_matrix\n",
    "# transform target variable\n",
    "y_matrix = multilabel_binarizer.transform(df_combined['categories'])\n",
    "print(df_combined['text'][2000])\n",
    "print(onehot_to_labels(y_matrix[2000]))\n",
    "y_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26088, 321)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_crafted_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 9s, sys: 1.14 s, total: 1min 10s\n",
      "Wall time: 1min 10s\n",
      "CPU times: user 7.26 s, sys: 3.13 s, total: 10.4 s\n",
      "Wall time: 10.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=GaussianNB(priors=None, var_smoothing=1e-09),\n",
       "          n_jobs=None)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(X_matrix, y_matrix, test_size=0.0, random_state=9)\n",
    "\n",
    "# Binary Relevance\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Performance metric\n",
    "\n",
    "# runtag=\"MuLaBnb\"\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "ml_m = LogisticRegression()\n",
    "\n",
    "clf1 = OneVsRestClassifier(ml_m)\n",
    "# fit model on train data\n",
    "%time clf1.fit(features_train, labels_train)\n",
    "\n",
    "\n",
    "# from xgboost import XGBClassifier\n",
    "# ml_m = XGBClassifier()\n",
    "\n",
    "# from sklearn.svm import LinearSVC\n",
    "# ml_m = LinearSVC(C=1)\n",
    "\n",
    "# from sklearn.naive_bayes import BernoulliNB\n",
    "# ml_m = BernoulliNB()\n",
    "\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# ml_m = RandomForestClassifier(random_state=1, n_estimators=10, min_samples_split=4, min_samples_leaf=2)\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "ml_m = GaussianNB()\n",
    "\n",
    "clf2 = OneVsRestClassifier(ml_m)\n",
    "# fit model on train data\n",
    "%time clf2.fit(features_train, labels_train)\n",
    "\n",
    "# from sklearn.metrics import f1_score\n",
    "# print(f1_score(labels_test, y_pred, average=\"micro\"))\n",
    "\n",
    "# print('Accuracy of BernoulliNB classifier on FULL set with oversampling for priority (Train set): {:.4f}'\n",
    "#      .format(clf.score(features_train, labels_train)))\n",
    "# print('Accuracy of BernoulliNB classifier on FULL set with oversampling for priority (Test set): {:.4f}'\n",
    "#      .format(clf.score(features_test, labels_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019b Submission (on 2019btrecis-test set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11488 Not again🙆🏼‍♂️🙆🏼‍♂️‼️‼️ https://t.co/1ygBJKNgCi\n",
      "12181 No, not again 🙆🏾‍♂️ https://t.co/SYwqwcn7Xj\n",
      "(15000, 321)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.02275683,  0.05240015,  0.02972024, ...,  0.        ,\n",
       "         0.        ,  0.04761905],\n",
       "       [ 0.01183188,  0.01825785, -0.01675739, ...,  0.        ,\n",
       "         0.        ,  0.04761905],\n",
       "       [-0.00803777,  0.00876423,  0.00747698, ...,  0.        ,\n",
       "         0.        ,  0.04761905],\n",
       "       ...,\n",
       "       [-0.00566667,  0.00825524,  0.00452569, ...,  0.        ,\n",
       "         0.        ,  0.0952381 ],\n",
       "       [ 0.03183927,  0.01161456, -0.00593295, ...,  0.1       ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.00541297,  0.02367819, -0.03326358, ...,  0.        ,\n",
       "         0.        ,  0.19047619]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct the feature boosting matrix for 2019a test set\n",
    "results=[]\n",
    "import json\n",
    "with open(\"test_dataset_2019b.json\") as f:\n",
    "    for line in f:\n",
    "        results.append(json.loads(line.strip()))\n",
    "df_test_2019b=pd.DataFrame(results)\n",
    "\n",
    "X_test_2019b_word2vec_matrix=to_matrix(df_test_2019b['text'])\n",
    "import os\n",
    "if os.path.isfile(\"trecis2019b_fbtest_matrix.csv\"):\n",
    "    X_test_2019b_fb=pd.read_csv(\"trecis2019b_fbtest_matrix.csv\")\n",
    "else:\n",
    "    feature_names=[\"hashtag_count\",\"sentiment_cpd\",\"sentiment_neg\",\"sentiment_neu\",\n",
    "               \"sentiment_pos\",\"is_action1\",\"is_action2\",\"is_action3\",\"is_action4\",\n",
    "              \"is_action5\",\"word_length\",\"char_length\",\"url_count\",\"digital_count\",\n",
    "              \"is_retweet\",\"caps_ratio\",\"at_count\",\"exclaim_count\",\"question_count\",\n",
    "              \"colon_check\",\"ner_count\"]\n",
    "    trecis2019b_fbtest_matrix=pd.DataFrame(columns=feature_names)\n",
    "    texts=df_test_2019b['text']\n",
    "    for index,tweet_text in enumerate(texts):\n",
    "        if index%1000==0:\n",
    "            print(\"Now is processing at index\",index)\n",
    "        features=return_features(tweet_text)\n",
    "        trecis2019b_fbtest_matrix.loc[len(trecis2019b_fbtest_matrix)]=features\n",
    "    trecis2019b_fbtest_matrix.to_csv(\"trecis2019b_fbtest_matrix.csv\",index=False)\n",
    "    \n",
    "X_test_2019b_fb=normalize_by_columns(X_test_2019b_fb)\n",
    "X_test_2019b=np.concatenate([X_test_2019b_word2vec_matrix, X_test_2019b_fb.iloc[:,0:21].values], axis=1)\n",
    "print(X_test_2019b.shape)\n",
    "X_test_2019b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0.51 # this is set emprically\n",
    "predicted_test1_proba=clf1.predict_proba(X_test_2019b)\n",
    "predicted_test1_proba=[normalize_prob(each) for each in predicted_test1_proba]\n",
    "predicted_test1_proba=np.array(predicted_test1_proba)\n",
    "\n",
    "predicted_test2_proba=clf2.predict_proba(X_test_2019b)\n",
    "predicted_test2_proba=[normalize_prob(each) for each in predicted_test2_proba]\n",
    "predicted_test2_proba=np.array(predicted_test2_proba)\n",
    "combined_predict_proba=(predicted_test1_proba+predicted_test2_proba)/2\n",
    "y_pred_prob_2019=combined_predict_proba\n",
    "predicted_2019 = (y_pred_prob_2019 >= t).astype(int)\n",
    "\n",
    "# prediction for priority\n",
    "predicted_pri = pri_model.predict(X_test_2019b)\n",
    "# predicted\n",
    "predictd_labels_2019=multilabel_binarizer.inverse_transform(predicted_2019)\n",
    "# predictd_labels_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ThirdPartyObservation</td>\n",
       "      <td>3801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FirstPartyObservation</td>\n",
       "      <td>2887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ServiceAvailable</td>\n",
       "      <td>1491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SearchAndRescue</td>\n",
       "      <td>509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MovePeople</td>\n",
       "      <td>919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Official</td>\n",
       "      <td>4593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MultimediaShare</td>\n",
       "      <td>3988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Location</td>\n",
       "      <td>5521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>InformationWanted</td>\n",
       "      <td>825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>News</td>\n",
       "      <td>7653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Advice</td>\n",
       "      <td>5899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Weather</td>\n",
       "      <td>893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Sentiment</td>\n",
       "      <td>2756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Discussion</td>\n",
       "      <td>8671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Factoid</td>\n",
       "      <td>6501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CleanUp</td>\n",
       "      <td>953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Donations</td>\n",
       "      <td>1091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Hashtags</td>\n",
       "      <td>5124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>EmergingThreats</td>\n",
       "      <td>5635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ContextualInformation</td>\n",
       "      <td>7451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>GoodsServices</td>\n",
       "      <td>559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Volunteer</td>\n",
       "      <td>743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>6973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>OriginalEvent</td>\n",
       "      <td>4983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NewSubEvent</td>\n",
       "      <td>4001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Category  Count\n",
       "0   ThirdPartyObservation   3801\n",
       "1   FirstPartyObservation   2887\n",
       "2        ServiceAvailable   1491\n",
       "3         SearchAndRescue    509\n",
       "4              MovePeople    919\n",
       "5                Official   4593\n",
       "6         MultimediaShare   3988\n",
       "7                Location   5521\n",
       "8       InformationWanted    825\n",
       "9                    News   7653\n",
       "10                 Advice   5899\n",
       "11                Weather    893\n",
       "12              Sentiment   2756\n",
       "13             Discussion   8671\n",
       "14                Factoid   6501\n",
       "15                CleanUp    953\n",
       "16              Donations   1091\n",
       "17               Hashtags   5124\n",
       "18        EmergingThreats   5635\n",
       "19  ContextualInformation   7451\n",
       "20          GoodsServices    559\n",
       "21              Volunteer    743\n",
       "22             Irrelevant   6973\n",
       "23          OriginalEvent   4983\n",
       "24            NewSubEvent   4001"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predicted_categories=[]\n",
    "all_predicted_categories = sum([list(tup) for tup in predictd_labels_2019],[])\n",
    "len(set(all_predicted_categories))\n",
    "\n",
    "all_predicted_categories_freq = nltk.FreqDist(all_predicted_categories)\n",
    "# create dataframe\n",
    "all_predicted_categories_df = pd.DataFrame({'Category': list(all_predicted_categories_freq.keys()), \n",
    "                              'Count': list(all_predicted_categories_freq.values())})\n",
    "all_predicted_categories_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000,)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_2019b[\"event_id\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission for UCDbaseline is done\n"
     ]
    }
   ],
   "source": [
    "runtag=\"baseline\"\n",
    "runlabel=\"UCD\"+runtag\n",
    "\n",
    "def normalize_priority(raw_priority=['0','1','2','3']):\n",
    "    raw_priority=[float(i) for i in raw_priority]\n",
    "    max_priority_score=np.max(raw_priority)\n",
    "    min_priority_score=np.min(raw_priority)\n",
    "    normalized=[]\n",
    "    for each in raw_priority:\n",
    "        normalized.append((each+1)/(max_priority_score-min_priority_score))\n",
    "#         normalized.append(0.75*each/(max_priority_score-min_priority_score)+(0.25*max_priority_score-min_priority_score)/(max_priority_score-min_priority_score))\n",
    "    return normalized\n",
    "import math\n",
    "short2longITs2019b = {'Advice': 'Other-Advice',\n",
    "                      'CleanUp': 'Report-CleanUp',\n",
    "                      'ContextualInformation': 'Other-ContextualInformation',\n",
    "                      'Discussion': 'Other-Discussion',\n",
    "                      'Donations': 'CallToAction-Donations',\n",
    "                      'EmergingThreats': 'Report-EmergingThreats',\n",
    "                      'Factoid': 'Report-Factoid',\n",
    "                      'FirstPartyObservation': 'Report-FirstPartyObservation',\n",
    "                      'GoodsServices': 'Request-GoodsServices',\n",
    "                      'Hashtags': 'Report-Hashtags',\n",
    "                      'InformationWanted': 'Request-InformationWanted',\n",
    "                      'Location': 'Report-Location',\n",
    "                      'MovePeople': 'CallToAction-MovePeople',\n",
    "                      'MultimediaShare': 'Report-MultimediaShare',\n",
    "                      'NewSubEvent': 'Report-NewSubEvent',\n",
    "                      'News': 'Report-News',\n",
    "                      'Official': 'Report-Official',\n",
    "                      'OriginalEvent': 'Report-OriginalEvent',\n",
    "                      'SearchAndRescue': 'Request-SearchAndRescue',\n",
    "                      'Sentiment': 'Other-Sentiment',\n",
    "                      'Irrelevant': 'Report-Irrelevant',\n",
    "                      'ServiceAvailable': 'Report-ServiceAvailable',\n",
    "                      'ThirdPartyObservation': 'Report-ThirdPartyObservation',\n",
    "                      'Volunteer': 'CallToAction-Volunteer',\n",
    "                      'Weather': 'Report-Weather'}\n",
    "\n",
    "event2test = {\"albertaWildfires2019\": \"TRECIS-CTIT-H-Test-029\",\n",
    "                          \"cycloneKenneth2019\": \"TRECIS-CTIT-H-Test-030\",\n",
    "                          \"philippinesEarthquake2019\": \"TRECIS-CTIT-H-Test-031\",\n",
    "                          \"coloradoStemShooting2019\": \"TRECIS-CTIT-H-Test-032\",\n",
    "                          \"southAfricaFloods2019\": \"TRECIS-CTIT-H-Test-033\",\n",
    "                          \"sandiegoSynagogueShooting2019\": \"TRECIS-CTIT-H-Test-034\"}\n",
    "\n",
    "# A naive approach: based on the averaged priority weights computed by information types in training set\n",
    "info_type_priority_weight_dict = {'Advice': 0.22862577231414025, 'CleanUp': 0.35653795010024114,\n",
    "                                  'News': 0.3270722902848146, 'Discussion': 0.19307661376501264,\n",
    "                                  'Donations': 0.3612767541359874, 'EmergingThreats': 0.8537984236364585,\n",
    "                                  'Factoid': 0.48917308056907677, 'FirstPartyObservation': 0.1420197717109446,\n",
    "                                  'GoodsServices': 0.6959346525995092, 'Hashtags': 0.2687451987058043,\n",
    "                                  'InformationWanted': 0.6688596972384949, 'Irrelevant': 0.0010061402766415601,\n",
    "                                  'OriginalEvent': 0.09192532381788204, 'MovePeople': 0.9253136744180216,\n",
    "                                  'MultimediaShare': 0.3128463518966703, 'Official': 0.6785618345463643,\n",
    "                                  'ContextualInformation': 0.1405539758609653, 'SearchAndRescue': 1.0010061402766415,\n",
    "                                  'Sentiment': 0.039519645594885515, 'ServiceAvailable': 0.7669372401298534,\n",
    "                                  'NewSubEvent': 0.9176682650090996, 'ThirdPartyObservation': 0.2703955630411556,\n",
    "                                  'Location': 0.03711651480530716, 'Volunteer': 0.3839639462617982,\n",
    "                                  'Weather': 0.33196659834790165}\n",
    "\n",
    "# priority prediction\n",
    "normalized_predicted_pri=normalize_priority(predicted_pri)\n",
    "\n",
    "info_type_priority_weight_list=list(info_type_priority_weight_dict.values())\n",
    "max_pri=np.max([float(i) for i in info_type_priority_weight_list])\n",
    "min_pri=np.min([float(i) for i in info_type_priority_weight_list])\n",
    "\n",
    "# Step 2: Convert the submission file to the standard format\n",
    "# generate submission file\n",
    "# import core.label2code as l2c\n",
    "# index2label = l2c.index2label\n",
    "str_pre = \"\"\n",
    "\n",
    "import random\n",
    "sub_dict={}\n",
    "for index,each in enumerate(predictd_labels_2019):\n",
    "    each=list(each)\n",
    "    if len(each)==0:\n",
    "        each=['Irrelevant']\n",
    "    elif len(each)>1 and 'Irrelevant' in each:\n",
    "        each.remove('Irrelevant')\n",
    "    elif 'Unknown' in each:\n",
    "        each.remove('Unknown')\n",
    "        \n",
    "    p=np.mean([float(info_type_priority_weight_dict[e]) for e in each])\n",
    "#     p = float(info_type_priority_weight_dict[index2label[each]])\n",
    "    # low=0.25, medium=0.5, high=0.75 and critical=1.0. \n",
    "    # equation for priority calculation where p is parameter =0.5 by default\n",
    "    # p*predicted_score+(1-p)*statistic_weight\n",
    "    parameter_p=0.5\n",
    "    predicted_score=normalized_predicted_pri[index]\n",
    "#   statistic_weight=normalize_priority([p])[0]\n",
    "    statistic_weight=p/(max_pri-min_pri)\n",
    "#     statistic_weight=0.75*p/(max_pri-min_pri)+(0.25*max_pri-min_pri)/(max_pri-min_pri)\n",
    "    p_final=parameter_p*predicted_score+(1-parameter_p)*statistic_weight\n",
    "\n",
    "    event_id=df_test_2019b.iloc[index]['event_id']\n",
    "    test_id=event2test[event_id]\n",
    "    post_id=df_test_2019b.iloc[index]['post_id']\n",
    "\n",
    "#     labels=[short2longITs2019A[index2label[each]] for each in multi_label_predicted[index]]\n",
    "    labels=[short2longITs2019b[e] for e in each]\n",
    "\n",
    "    if math.isnan(p_final):\n",
    "        p_final=0.0\n",
    "    if p_final>1.0:\n",
    "        p_final=1.0\n",
    "        \n",
    "    line_sub=test_id+\"\\tQ0\\t\"+str(post_id)+\"\\t\"+ \"#\" + \"\\t\" + str(p_final) +\"\\t\"+str(labels)+\"\\t\"+runlabel+ \"\\n\"\n",
    "#     str_pre +=line_sub\n",
    "    if event_id not in sub_dict:\n",
    "#         sub_dict[post_id]=[event_id]\n",
    "        sub_dict[event_id]={line_sub:p_final}\n",
    "    else:\n",
    "        sub_dict[event_id][line_sub]=p_final\n",
    "str_pre=\"\"\n",
    "\n",
    "for each in sub_dict:\n",
    "    rank_inevent=1\n",
    "    sub_inevent=sub_dict[each]\n",
    "    sub_inevent_sorted=sorted(sub_inevent.items(), key=lambda kv: kv[1],reverse=True)\n",
    "    for tup in sub_inevent_sorted:\n",
    "        line_sub=tup[0].replace(\"#\",str(rank_inevent))\n",
    "        str_pre+=line_sub\n",
    "        rank_inevent+=1\n",
    "        \n",
    "    \n",
    "with open(runlabel+\".txt\", \"w\") as f:\n",
    "    f.write(str_pre)\n",
    "\n",
    "print(\"Submission for \"+runlabel+\" is done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
